"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[1777],{3550:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"introduction","title":"Introduction","description":"BoTorch (pronounced \\"bow-torch\\" / \u02c8b\u014d-t\u022frch) is a library for","source":"@site/../docs/introduction.md","sourceDirName":".","slug":"/introduction","permalink":"/botorch/docs/next/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/pytorch/botorch/edit/main/docs/../docs/introduction.md","tags":[],"version":"current","lastUpdatedBy":"Cristian Lara","lastUpdatedAt":1734367864000,"frontMatter":{"id":"introduction","title":"Introduction"},"sidebar":"docs","next":{"title":"Design Philosophy","permalink":"/botorch/docs/next/design_philosophy"}}');var o=t(4848),a=t(8453);const r={id:"introduction",title:"Introduction"},s=void 0,c={},d=[{value:"Why BoTorch?",id:"why-botorch",level:2},{value:"Improved Developer Efficiency",id:"improved-developer-efficiency",level:3},{value:"State-of-the-art Modeling",id:"state-of-the-art-modeling",level:3},{value:"Harnessing the Features of PyTorch",id:"harnessing-the-features-of-pytorch",level:3},{value:"Bridging the Gap Between Research and Production",id:"bridging-the-gap-between-research-and-production",level:3},{value:"Target Audience",id:"target-audience",level:2}];function l(e){const n={a:"a",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",section:"section",sup:"sup",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.p,{children:['BoTorch (pronounced "bow-torch" / \u02c8b\u014d-t\u022frch) is a library for\n',(0,o.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Bayesian_optimization",children:"Bayesian Optimization"}),"\nresearch built on top of ",(0,o.jsx)(n.a,{href:"https://pytorch.org/",children:"PyTorch"}),", and is part of the\nPyTorch ecosystem. Read the ",(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/1910.06403",children:"BoTorch paper"}),"\n",(0,o.jsx)(n.sup,{children:(0,o.jsx)(n.a,{href:"#user-content-fn-botorch",id:"user-content-fnref-botorch","data-footnote-ref":"","aria-describedby":"footnote-label",children:"1"})})," for a detailed exposition."]}),"\n",(0,o.jsx)(n.p,{children:"Bayesian Optimization (BayesOpt) is an established technique for sequential\noptimization of costly-to-evaluate black-box functions. It can be applied to a\nwide variety of problems, including hyperparameter optimization for machine\nlearning algorithms, A/B testing, as well as many scientific and engineering\nproblems."}),"\n",(0,o.jsxs)(n.p,{children:["BoTorch is best used in tandem with ",(0,o.jsx)(n.a,{href:"https://ax.dev",children:"Ax"}),", Facebook's open-source\nadaptive experimentation platform, which provides an easy-to-use interface for\ndefining, managing and running sequential experiments, while handling\n(meta-)data management, transformations, and systems integration. Users who just\nwant an easy-to-use suite for Bayesian Optimization\n",(0,o.jsx)(n.a,{href:"https://ax.dev/docs/bayesopt",children:"should start with Ax"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"why-botorch",children:"Why BoTorch?"}),"\n",(0,o.jsx)(n.h3,{id:"improved-developer-efficiency",children:"Improved Developer Efficiency"}),"\n",(0,o.jsx)(n.p,{children:"BoTorch provides a modular and easily extensible interface for composing\nBayesian Optimization primitives, including probabilistic models, acquisition\nfunctions, and optimizers."}),"\n",(0,o.jsxs)(n.p,{children:['It significantly improves developer efficiency by utilizing quasi-Monte-Carlo\nacquisition functions (by way of the "re-parameterization trick"\n',(0,o.jsx)(n.sup,{children:(0,o.jsx)(n.a,{href:"#user-content-fn-autoencvarbayes",id:"user-content-fnref-autoencvarbayes","data-footnote-ref":"","aria-describedby":"footnote-label",children:"2"})}),", ",(0,o.jsx)(n.sup,{children:(0,o.jsx)(n.a,{href:"#user-content-fn-reparamacq",id:"user-content-fnref-reparamacq","data-footnote-ref":"","aria-describedby":"footnote-label",children:"3"})}),"), which makes it straightforward to implement\nnew ideas without having to impose restrictive assumptions about the underlying\nmodel. Specifically, it avoids pen and paper math to derive analytic expressions\nfor acquisition functions and their gradients.\nMore importantly, it opens the door for novel approaches that do not admit\nanalytic solutions, including batch acquisition functions and proper handling of\nrich multi-output models with multiple correlated outcomes."]}),"\n",(0,o.jsx)(n.p,{children:"BoTorch follows the same modular design philosophy as PyTorch, which makes it\nvery easy for users to swap out or rearrange individual components in order to\ncustomize all aspects of their algorithm, thereby empowering researchers to do\nstate-of-the art research on modern Bayesian Optimization methods."}),"\n",(0,o.jsx)(n.h3,{id:"state-of-the-art-modeling",children:"State-of-the-art Modeling"}),"\n",(0,o.jsxs)(n.p,{children:["Bayesian Optimization traditionally relies heavily on Gaussian Process (GP)\nmodels, which provide well-calibrated uncertainty estimates. BoTorch provides\nfirst-class support for state-of-the art probabilistic models in\n",(0,o.jsx)(n.a,{href:"https://gpytorch.ai",children:"GPyTorch"}),", a library for efficient and scalable GPs\nimplemented in PyTorch (and to which the BoTorch authors have significantly\ncontributed).\nThis includes support for multi-task GPs, deep kernel learning, deep GPs, and\napproximate inference. This enables using GP models for problems that have\ntraditionally not been amenable to Bayesian Optimization techniques."]}),"\n",(0,o.jsxs)(n.p,{children:["In addition, BoTorch's lightweight APIs are model-agnostic (they can for example\nwork with ",(0,o.jsx)(n.a,{href:"http://pyro.ai",children:"Pyro"})," models), and support optimization of\nacquisition functions over any kind of posterior distribution, as long as it can\nbe sampled from."]}),"\n",(0,o.jsx)(n.h3,{id:"harnessing-the-features-of-pytorch",children:"Harnessing the Features of PyTorch"}),"\n",(0,o.jsx)(n.p,{children:"Built on PyTorch, BoTorch takes advantage of auto-differentiation, native\nsupport for highly parallelized modern hardware (such as GPUs) using\ndevice-agnostic code, and a dynamic computation graph that facilitates\ninteractive development."}),"\n",(0,o.jsx)(n.p,{children:"BoTorch's modular design allows for a great deal of modeling flexibility for\nincluding deep and/or convolutional architectures through seamless integration\nwith generic PyTorch modules. Importantly, working full-stack in python allows\nback-propagating gradients through the full composite model, in turn enabling\njoint training of GP and Neural Network modules, and end-to-end gradient-based\noptimization of acquisition functions operating on differentiable models."}),"\n",(0,o.jsx)(n.h3,{id:"bridging-the-gap-between-research-and-production",children:"Bridging the Gap Between Research and Production"}),"\n",(0,o.jsxs)(n.p,{children:["BoTorch implements modular building blocks for modern Bayesian Optimization.\nIt bridges the gap between research and production by being a very flexible\nresearch framework, but at the same time, a reliable, production-grade\nimplementation that integrates well with other higher-level platforms,\nspecifically ",(0,o.jsx)(n.a,{href:"https://ax.dev",children:"Ax"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"target-audience",children:"Target Audience"}),"\n",(0,o.jsx)(n.p,{children:"The primary audience for hands-on use of BoTorch are researchers and\nsophisticated practitioners in Bayesian Optimization and AI."}),"\n",(0,o.jsxs)(n.p,{children:["We recommend using BoTorch as a low-level API for implementing new algorithms\nfor Ax. Ax has been designed to be an easy-to-use platform for end-users, which\nat the same time is flexible enough for Bayesian Optimization researchers to\nplug into for handling of feature transformations, (meta-)data management,\nstorage, etc. See ",(0,o.jsx)(n.a,{href:"botorch_and_ax",children:"Using BoTorch with Ax"})," for more details."]}),"\n",(0,o.jsx)(n.p,{children:"We recommend that end-users who are not actively doing research on Bayesian\nOptimization simply use Ax."}),"\n","\n",(0,o.jsxs)(n.section,{"data-footnotes":"",className:"footnotes",children:[(0,o.jsx)(n.h2,{className:"sr-only",id:"footnote-label",children:"Footnotes"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{id:"user-content-fn-botorch",children:["\n",(0,o.jsxs)(n.p,{children:["M. Balandat, B. Karrer, D. R. Jiang, S. Daulton, B. Letham, A. G. Wilson,\nand E. Bakshy. BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization.\nAdvances in Neural Information Processing Systems 33, 2020.\n",(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/1910.06403",children:"pdf"})," ",(0,o.jsx)(n.a,{href:"#user-content-fnref-botorch","data-footnote-backref":"","aria-label":"Back to reference 1",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{id:"user-content-fn-autoencvarbayes",children:["\n",(0,o.jsxs)(n.p,{children:["D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes.\nArXiv e-prints, ",(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/1312.6114",children:"arXiv:1312.6114"}),", Dec 2013. ",(0,o.jsx)(n.a,{href:"#user-content-fnref-autoencvarbayes","data-footnote-backref":"","aria-label":"Back to reference 2",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{id:"user-content-fn-reparamacq",children:["\n",(0,o.jsxs)(n.p,{children:["J. T. Wilson, R. Moriconi, F. Hutter, and M. P. Deisenroth.\nThe reparameterization trick for acquisition functions. ArXiv e-prints,\n",(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/1712.00424",children:"arXiv:1712.00424"}),", Dec 2017. ",(0,o.jsx)(n.a,{href:"#user-content-fnref-reparamacq","data-footnote-backref":"","aria-label":"Back to reference 3",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var i=t(6540);const o={},a=i.createContext(o);function r(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);