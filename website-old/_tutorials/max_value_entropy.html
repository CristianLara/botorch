
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<div class="notebook">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-max-value-entropy-search-acquisition-function">The max value entropy search acquisition function<a class="anchor-link" href="#The-max-value-entropy-search-acquisition-function">¶</a></h2><p>Max-value entropy search (MES) acquisition function quantifies the information gain about the maximum of a black-box function by observing this black-box function $f$ at the candidate set $\{\textbf{x}\}$ (see [1, 2]). BoTorch provides implementations of the MES acquisition function and its multi-fidelity (MF) version with support for trace observations. In this tutorial, we explain at a high level how the MES acquisition function works, its implementation in BoTorch and how to use the MES acquisition function to query the next point in the optimization process.</p>
<p>In general, we recommend using <a href="https://ax.dev">Ax</a> for a simple BO setup like this one, since this will simplify your setup (including the amount of code you need to write) considerably. You can use a custom BoTorch model and acquisition function in Ax, following the <a href="./custom_botorch_model_in_ax">Using BoTorch with Ax</a> tutorial. To use the MES acquisition function, it is sufficient to add <code>"botorch_acqf_class": qMaxValueEntropy,</code> to <code>model_kwargs</code>. The linked tutorial shows how to use a custom BoTorch model. If you'd like to let Ax choose which model to use based on the properties of the search space, you can skip the <code>surrogate</code> argument in <code>model_kwargs</code>.</p>
<h3 id="1.-MES-acquisition-function-for-$q=1$-with-noisy-observation">1. MES acquisition function for $q=1$ with noisy observation<a class="anchor-link" href="#1.-MES-acquisition-function-for-$q=1$-with-noisy-observation">¶</a></h3><p>For illustrative purposes, we focus in this section on the non-q-batch-mode case ($q=1$). We also assume that the evaluation of the black-box function is noisy. Let us first introduce some notation:</p>
<ul>
<li>$f^* = \max_\mathcal{X} (f(\textbf{x}))$, the maximum of the black-box function $f(\textbf{x})$ in the design space $\mathcal{X}$</li>
<li>$y = f(\textbf{x}) + \epsilon, \epsilon \sim N(0, \sigma^2_\epsilon)$, the noisy observation at the design point $\textbf{x}$</li>
<li>$h(Y) = \mathbb{E}_Y[-\log(p(y))] = -\int_\mathcal{Y} p(y)\log p(y) dy$, the differential entropy of random variable $Y$ with support $\mathcal{Y}$: the larger is $h(Y)$, the larger is the uncertainty of $Y$.</li>
<li>$v(\mathcal{D}) = -\mathbb{E}_D[h(F^*\mid\mathcal{D})]$, the value of data set $\mathcal{D}$, where $F^*$ denotes the function maximum (a random variable in our context of our model).</li>
</ul>
<p>The Max-value Entropy Search (MES) acquisition function at $\textbf{x}$ after observing $\mathcal{D}_t$ can be written as
\begin{align}
    \alpha_{\text{MES}}(\textbf{x}) 
    &amp;= v(\mathcal{D}_t\cup \{(\textbf{x}, y)\}) - v(\mathcal{D}_t) \\
    &amp;= - \mathbb{E}_Y[h(F^* \mid \mathcal{D}_t\cup \{(\textbf{x}, Y)\})] + h(F^*\mid\mathcal{D}_t) \\
    &amp;= - \mathbb{E}_Y[h(F^* \mid Y)] + h(F^*) \\
    &amp;= I(F^*; Y) \\
    &amp;= I(Y; F^*) \quad \text{(symmetry)} \\
    &amp;= - \mathbb{E}_{F^*}[h(Y \mid F^*)] + h(Y) \\    
\end{align}
, which is the mutual information of random variables
$F^*\mid \mathcal{D}_t$ and $Y \mid \textbf{x}, \mathcal{D}_t$.
Here $F^*$ follows the max value distribution conditioned on $\mathcal{D}_t$, and $Y$ follows the GP posterior distribution with noise at $\textbf{x}$ after observing $\mathcal{D}_t$.</p>
<p>Rewrite the above formula as
\begin{align}
    \alpha_{\text{MES}}(\textbf{x}) &amp;= - H_1 + H_0, \\
    H_0 &amp;= h(Y) = \log \left(\sqrt{2\pi e (\sigma_f^2 + \sigma_\epsilon^2)}\right) \\
    H_1 &amp;= \mathbb{E}_{F^*}[h(Y \mid F^*)] \\
        &amp;\simeq \frac{1}{\left|\mathcal{F}_*\right|} \Sigma_{\mathcal{F}_*} h(Y\mid f^*))
\end{align}
, where $\mathcal{F}_*$ are the max value samples drawn from the posterior after observing $\mathcal{D}_t$. Without noise, $p(y \mid f^*) = p(f \mid f \leq f^*)$ is a truncated normal distribution with an analytic expression for its entropy. With noise, $Y\mid F\leq f^*$ is not a truncated normal distribution anymore. The question is then how to compute $h(Y\mid f^*)$ or equivalently $p(y\mid f \leq f^*)$?</p>
<p>Using Bayes' theorem,
\begin{align}
    p(y\mid f \leq f^*) = \frac{P(f \leq f^* \mid y) p(y)}{P(f \leq f^* )}
\end{align}
, where</p>
<ul>
<li>$p(y)$ is the posterior probability density function (PDF) with observation noise.</li>
<li>$P(f \leq f^*)$ is the posterior cummulative distribution function (CDF) without observation noise, given any $f^*$.</li>
</ul>
<p>We also know from the GP predictive distribution
\begin{align}
    \begin{bmatrix}
        y \\ f
    \end{bmatrix}
    \sim \mathcal{N} \left(
    \begin{bmatrix}
        \mu \\ \mu
    \end{bmatrix} , 
    \begin{bmatrix}
        \sigma_f^2 + \sigma_\epsilon^2 &amp; \sigma_f^2 \\ 
        \sigma_f^2 &amp; \sigma_f^2
    \end{bmatrix}
    \right).
\end{align}
So
\begin{align}
    f \mid y \sim \mathcal{N} (u, s^2)
\end{align}
, where
\begin{align}
    u   &amp;= \frac{\sigma_f^2(y-\mu)}{\sigma_f^2 + \sigma_\epsilon^2} + \mu \\
    s^2 &amp;= \sigma_f^2 - \frac{(\sigma_f^2)^2}{\sigma_f^2 + \sigma_\epsilon^2}
        = \frac{\sigma_f^2\sigma_\epsilon^2}{\sigma_f^2 + \sigma_\epsilon^2}
\end{align}
Thus, $P(f \leq f^* \mid y)$ is the CDF of above Gaussian.</p>
<p>Finally, given $f^*$, we have<br/>
\begin{align}
    h(Y \mid f^*) 
    &amp;= -\int_\mathcal{Y} p(y \mid f^*)\log(p(y \mid f^*)) dy\\
    &amp;= -\int_\mathcal{Y} Zp(y)\log(Zp(y)) dy \\
    &amp;\simeq -\frac{1}{\left|\mathcal{Y}\right|} \Sigma_{\mathcal{Y}} Z\log(Zp(y)), \\
    Z &amp;= \frac{P(f \leq f^* \mid y)}{P(f \leq f^* )}
\end{align}
, where $Z$ is the ratio of two CDFs and $\mathcal{Y}$ is the samples drawn from the posterior distribution with noisy observation. The above formulation for noisy MES is inspired from the MF-MES formulation proposed by Takeno <em>et. al</em> [1], which is essentially the same as what is outlined above.</p>
<p>Putting all together,
\begin{align}
    \alpha_{\text{MES}}(\textbf{x}) 
    &amp;= H_0 - H_1 \\
    &amp;\simeq H_0 - H_1^{MC}\\
    &amp;= \log \left(\sqrt{2\pi e (\sigma_f^2 + \sigma_\epsilon^2)}\right) + \frac{1}{\left|\mathcal{F}^*\right|} \Sigma_{\mathcal{F}^*} \frac{1}{\left|\mathcal{Y}\right|} \Sigma_{\mathcal{Y}} (Z\log Z + Z\log p(y))
\end{align}</p>
<p>The next design point to query is chosen as the point that maximizes this aquisition function, <em>i. e.</em>,
\begin{align}
    \textbf{x}_{\text{next}} = \max_{\textbf{x} \in \mathcal{X}} \alpha_{\text{MES}}(\textbf{x})
\end{align}</p>
<p>The implementation in Botorch basically follows the above formulation for both non-MF and MF cases. One difference is that, in order to reduce the variance of the MC estimator for $H_1$, we apply also regression adjustment to get an estimation of $H_1$,
\begin{align}
    \widehat{H}_1 &amp;= H_1^{MC} - \beta (H_0^{MC} - H_0) 
\end{align}
, where
\begin{align}
    H_0^{MC} &amp;= - \frac{1}{\left|\mathcal{Y}\right|} \Sigma_{\mathcal{Y}} \log p(y) \\
    \beta &amp;= \frac{Cov(h_1, h_0)}{\sqrt{Var(h_1)Var(h_0)}} \\
    h_0 &amp;= -\log p(y) \\
    h_1 &amp;= -Z\log(Zp(y)) \\
\end{align}
This turns out to reduce the variance of the acquisition value by a significant factor, especially when the acquisition value is small, hence making the algorithm numerically more stable.</p>
<p>For the case of $q &gt; 1$, joint optimization becomes difficult, since the q-batch-mode MES acquisiton function becomes not tractable due to the multivariate normal CDF functions in $Z$. Instead, the MES acquisition optimization is solved sequentially and using fantasies, <em>i. e.</em>, we generate one point each time and when we try to generate the $i$-th point, we condition the models on the $i-1$ points generated prior to this (using the $i-1$ points as fantasies).</p>
<br/>
__References__

<p>[1] <a href="https://arxiv.org/abs/1901.08275">Takeno, S., et al., <em>Multi-fidelity Bayesian Optimization with Max-value Entropy Search.</em>  arXiv:1901.08275v1, 2019</a></p>
<p>[2] <a href="https://arxiv.org/abs/1703.01968">Wang, Z., Jegelka, S., <em>Max-value Entropy Search for Efficient Bayesian Optimization.</em> arXiv:1703.01968v3, 2018</a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.-Setting-up-a-toy-model">2. Setting up a toy model<a class="anchor-link" href="#2.-Setting-up-a-toy-model">¶</a></h3><p>We will fit a standard SingleTaskGP model on noisy observations of the synthetic 2D Branin function on the hypercube $[-5,10]\times [0, 15]$.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [1]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">botorch.test_functions</span> <span class="kn">import</span> <span class="n">Branin</span>
<span class="kn">from</span> <span class="nn">botorch.fit</span> <span class="kn">import</span> <span class="n">fit_gpytorch_mll</span>
<span class="kn">from</span> <span class="nn">botorch.models</span> <span class="kn">import</span> <span class="n">SingleTaskGP</span>
<span class="kn">from</span> <span class="nn">botorch.utils.transforms</span> <span class="kn">import</span> <span class="n">standardize</span><span class="p">,</span> <span class="n">normalize</span>
<span class="kn">from</span> <span class="nn">gpytorch.mlls</span> <span class="kn">import</span> <span class="n">ExactMarginalLogLikelihood</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>

<span class="n">bounds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">Branin</span><span class="o">.</span><span class="n">_bounds</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">train_X</span> <span class="o">=</span> <span class="n">bounds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">bounds</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">bounds</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">train_Y</span> <span class="o">=</span> <span class="n">Branin</span><span class="p">(</span><span class="n">negate</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">train_X</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">train_X</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">)</span>
<span class="n">train_Y</span> <span class="o">=</span> <span class="n">standardize</span><span class="p">(</span><span class="n">train_Y</span> <span class="o">+</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">train_Y</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SingleTaskGP</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">)</span>
<span class="n">mll</span> <span class="o">=</span> <span class="n">ExactMarginalLogLikelihood</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="n">fit_gpytorch_mll</span><span class="p">(</span><span class="n">mll</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.-Defining-the-MES-acquisition-function">3. Defining the MES acquisition function<a class="anchor-link" href="#3.-Defining-the-MES-acquisition-function">¶</a></h3><p>The <code>qMaxValueEntropy</code> acquisition function is a subclass of <code>MCAcquisitionFunction</code> and supports pending points <code>X_pending</code>. Required arguments for the constructor are <code>model</code> and <code>candidate_set</code> (the discretized candidate points in the design space that will be used to draw max value samples). There are also other optional parameters, such as number of max value samples $\mathcal{F^*}$, number of $\mathcal{Y}$ samples and number of fantasies (in case of $q&gt;1$). Two different sampling algorithms are supported for the max value samples: the discretized Thompson sampling and the Gumbel sampling introduced in [2]. Gumbel sampling is the default choice in the acquisition function.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [2]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">botorch.acquisition.max_value_entropy_search</span> <span class="kn">import</span> <span class="n">qMaxValueEntropy</span>

<span class="n">candidate_set</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span>
    <span class="mi">1000</span><span class="p">,</span> <span class="n">bounds</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">bounds</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">bounds</span><span class="o">.</span><span class="n">dtype</span>
<span class="p">)</span>
<span class="n">candidate_set</span> <span class="o">=</span> <span class="n">bounds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">bounds</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">bounds</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">candidate_set</span>
<span class="n">qMES</span> <span class="o">=</span> <span class="n">qMaxValueEntropy</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">candidate_set</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="4.-Optimizing-the-MES-acquisition-function-to-get-the-next-candidate-points">4. Optimizing the MES acquisition function to get the next candidate points<a class="anchor-link" href="#4.-Optimizing-the-MES-acquisition-function-to-get-the-next-candidate-points">¶</a></h3><p>In order to obtain the next candidate point(s) to query, we need to optimize the acquisition function over the design space. For $q=1$ case, we can simply call the <code>optimize_acqf</code> function in the library. At $q&gt;1$, due to the intractability of the aquisition function in this case, we need to use either sequential or cyclic optimization (multiple cycles of sequential optimization).</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [3]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">botorch.optim</span> <span class="kn">import</span> <span class="n">optimize_acqf</span>

<span class="c1"># for q = 1</span>
<span class="n">candidates</span><span class="p">,</span> <span class="n">acq_value</span> <span class="o">=</span> <span class="n">optimize_acqf</span><span class="p">(</span>
    <span class="n">acq_function</span><span class="o">=</span><span class="n">qMES</span><span class="p">,</span>
    <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
    <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">num_restarts</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">raw_samples</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">candidates</span><span class="p">,</span> <span class="n">acq_value</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[3]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>(tensor([[1.5350, 0.0758]]), tensor(0.0121))</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [4]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># for q = 2, sequential optimization</span>
<span class="n">candidates_q2</span><span class="p">,</span> <span class="n">acq_value_q2</span> <span class="o">=</span> <span class="n">optimize_acqf</span><span class="p">(</span>
    <span class="n">acq_function</span><span class="o">=</span><span class="n">qMES</span><span class="p">,</span>
    <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
    <span class="n">q</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">num_restarts</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">raw_samples</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">sequential</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">candidates_q2</span><span class="p">,</span> <span class="n">acq_value_q2</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[4]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>(tensor([[-0.3238,  0.6565],
         [ 1.5349,  0.0748]]), tensor([0.0135, 0.0065]))</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [5]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">botorch.optim</span> <span class="kn">import</span> <span class="n">optimize_acqf_cyclic</span>

<span class="c1"># for q = 2, cyclic optimization</span>
<span class="n">candidates_q2_cyclic</span><span class="p">,</span> <span class="n">acq_value_q2_cyclic</span> <span class="o">=</span> <span class="n">optimize_acqf_cyclic</span><span class="p">(</span>
    <span class="n">acq_function</span><span class="o">=</span><span class="n">qMES</span><span class="p">,</span>
    <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
    <span class="n">q</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">num_restarts</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">raw_samples</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">cyclic_options</span><span class="o">=</span><span class="p">{</span><span class="s2">"maxiter"</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
<span class="p">)</span>
<span class="n">candidates_q2_cyclic</span><span class="p">,</span> <span class="n">acq_value_q2_cyclic</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt output_prompt">Out[5]:</div>
<div class="output_text output_subarea output_execute_result">
<pre>(tensor([[-0.3236,  0.6563],
         [ 1.5326,  0.0732]]), tensor([0.0101, 0.0064]))</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The use of the <code>qMultiFidelityMaxValueEntropy</code> acquisition function is very similar to <code>qMaxValueEntropy</code>, but requires additional optional arguments related to the fidelity and cost models. We will provide more details on the MF-MES acquisition function in a separate tutorial.</p>
</div>
</div>
</div>
</div>