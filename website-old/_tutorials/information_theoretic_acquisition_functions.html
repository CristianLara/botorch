
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<div class="notebook">
<div class="cell border-box-sizing text_cell rendered" id="cell-id=738920c6"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Information-theoretic-acquisition-functions">Information-theoretic acquisition functions<a class="anchor-link" href="#Information-theoretic-acquisition-functions">¶</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=c6b5d9a9"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This notebook illustrates the use of some information-theoretic acquisition functions in BoTorch for single and multi-objective optimization. We present a single-objective example in section 1 and a multi-objective example in section 2. Before introducing these examples, we present an overview on the different approaches and how they are estimated.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=876c4359"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Notation">Notation<a class="anchor-link" href="#Notation">¶</a></h2><p>We consider the problem of maximizing a function $f: \mathbb{X} \rightarrow \mathbb{R}^M$. In the single-objective setting ($M=1$), the maximum is defined as usual with respect to the total ordering over the real numbers. In the multi-objective setting ($M&gt;1$), the maximum is defined with respect to the Pareto partial ordering over vectors. By an abuse in notation, we denote the optimal set of inputs and outputs by</p>
<p>$$\mathbb{X}^* = \text{arg}\max_{\mathbf{x} \in \mathbb{X}} f(\mathbf{x}) \subseteq \mathbb{X} \quad \text{and} \quad \mathbb{Y}^* = f(\mathbb{X}^*) = \max_{\mathbf{x} \in \mathbb{X}} f(\mathbf{x}) \subset \mathbb{R}^M,$$</p>
<p>respectively for both the single and multi-objective setting. We denote the collection of optimal input-output pairs by $(\mathbb{X}^*, \mathbb{Y}^*)$.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=1499a1cd"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Information-theoretic-acquisition-functions">Information-theoretic acquisition functions<a class="anchor-link" href="#Information-theoretic-acquisition-functions">¶</a></h2><p>Information-theoretic (IT) acquisition functions work by quantifying the utility of an input $\mathbf{x} \in \mathbb{X}$ based on how "informative" the corresponding observation $\mathbf{y} \in \mathbb{R}^M$ will be in learning more about the distribution of some statistic of the function $S(f)$. Here, we define the notion of information via the mutual information ($\text{MI}$):</p>
<p>\begin{equation}
    \alpha^{\text{IT}}(\mathbf{x}|D_n) 
    = \text{MI}(\mathbf{y}; S(f)| \mathbf{x}, D_n) 
    = H[p(\mathbf{y}|D_n)] - \mathbb{E}_{p(S(f)|D_n)}[H[p(\mathbf{y}| \mathbf{x}, D_n, S(f)]],
\end{equation}</p>
<p>where $D_n = \{(\mathbf{x}_t, \mathbf{y}_t)\}_{t=1,\dots,n}$ denotes the data set of sampled inputs and observations and the function $H$ denotes the differential entropy $H[p(\mathbf{x})] = - \int p(\mathbf{x}) \log(p(\mathbf{x})) d\mathbf{x}$. The main difference between existing information-theoretic acquisition functions in the literature is the choice of statistic $S$ and the modelling assumptions that are made in order to estimate the resulting acquisition function. In this notebook, we focus on three particular cases of information-theoretic acquisition functions:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=7218d85d"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Predictive-Entropy-Search-(PES)">Predictive Entropy Search (PES)<a class="anchor-link" href="#Predictive-Entropy-Search-(PES)">¶</a></h3><p>The PES acquisition function [1] considers the problem of learning more about the distribution of the optimal inputs: $S(f) = \mathbb{X}^*$.</p>
<p>\begin{equation}
\alpha^{\text{PES}}(\mathbf{x}|D_n) 
= \text{MI}(\mathbf{y}; \mathbb{X}^*| \mathbf{x}, D_n) 
= H[p(\mathbf{y}|D_n)] - \mathbb{E}_{p(\mathbb{X}^*|D_n)}[H[p(\mathbf{y}| \mathbf{x}, D_n, \mathbb{X}^*)]].
\end{equation}</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=c7b1d071"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Max-value-Entropy-Search-(MES)">Max-value Entropy Search (MES)<a class="anchor-link" href="#Max-value-Entropy-Search-(MES)">¶</a></h3><p>The MES acquisition function [2] considers the problem of learning more about the distribution of the optimal outputs: $S(f) = \mathbb{Y}^*$.</p>
<p>\begin{equation}
\alpha^{\text{MES}}(\mathbf{x}|D_n) 
= \text{MI}(\mathbf{y}; \mathbb{Y}^*| \mathbf{x}, D_n) 
= H[p(\mathbf{y}|D_n)] - \mathbb{E}_{p(\mathbb{Y}^*|D_n)}[H[p(\mathbf{y}| \mathbf{x}, D_n, \mathbb{Y}^*)]].
\end{equation}</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=3c8e68ed"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Joint-Entropy-Search-(JES)">Joint Entropy Search (JES)<a class="anchor-link" href="#Joint-Entropy-Search-(JES)">¶</a></h3><p>The JES acquisition function [3] considers the problem of learning more about the distribution of the optimal inputs and outputs: $S(f) = (\mathbb{X}^*, \mathbb{Y}^*)$.</p>
<p>\begin{equation}
\alpha^{\text{JES}}(\mathbf{x}|D_n) 
= \text{MI}(\mathbf{y}; (\mathbb{X}^*, \mathbb{Y}^*)| \mathbf{x}, D_n) 
= H[p(\mathbf{y}|D_n)] - \mathbb{E}_{p((\mathbb{X}^*, \mathbb{Y}^*)|D_n)}[H[p(\mathbf{y}| \mathbf{x}, D_n, (\mathbb{X}^*, \mathbb{Y}^*))]].
\end{equation}</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=f14bbab1"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Estimation">Estimation<a class="anchor-link" href="#Estimation">¶</a></h2><p>In order to estimate the three acquistion functions listed above, we make two simplfying assumptions:</p>
<p><strong>[Assumption 1]</strong> We assume an independent Gaussian process prior on each objective function.</p>
<p><strong>[Assumption 2]</strong> We assume a Gaussian observation likelihood.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=c69dfe94"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="First-term">First term<a class="anchor-link" href="#First-term">¶</a></h3><p>Under the modelling assumptions, the first term in each of the acquisition functions is an entropy of a Gaussian random variable, which is analytically tractable.</p>
<h3 id="Second-term">Second term<a class="anchor-link" href="#Second-term">¶</a></h3><p>The second term in each of the acquisition functions is an expectation of an entropy over an intractable distribution. The expectation can be estimated using Monte Carlo, whilst the entropy has to be approximated using different strategies such as moment-matching.</p>
<p><strong>Monte Carlo.</strong> To sample from the distribution over the optimal points, we can first (approximately) sample a collection of posterior paths $f_j \sim p(f|D_n)$ and then optimize them to obtain the sample of optimal points $(\mathbb{X}^*_j, \mathbb{Y}^*_j)$ for $j=1,\dots,J$.</p>
<p><strong>PES entropy estimate.</strong> In <code>qPredictiveEntropySearch</code> and <code>qMultiObjectivePredictiveEntropySearch</code>, we approximate the entropy term arising in PES using the expectation propagation strategy described in [4]. In particular, we first relax the global optimality condition:</p>
<p>\begin{align}
    H[p(\mathbf{y}| \mathbf{x}, D_n, \mathbb{X}^*)]
    &amp;\overset{(1)}{=} H[p(\mathbf{y}| \mathbf{x}, D_n, f(\mathbb{X}) \preceq f(\mathbb{X}^*))]
    \\\\
    &amp;\overset{(2)}{\leq} H[p(\mathbf{y}| \mathbf{x}, D_n, f(X_n \cup \{\mathbf{x}\}) \preceq f(\mathbb{X}^*))].
\end{align}</p>
<p>(1) This statement follows from the observation that conditioning on the optimal points $\mathbb{X}^*$ is equivalent to knowing that all points lie below the objective values at the optimal inputs: $f(\mathbb{X}) \preceq f(\mathbb{X}^*)$.</p>
<p>(2) We replace the global optimality condition with the local optimality condition: $f(X_n \cup \{\mathbf{x}\}) \preceq f(\mathbb{X}^*)$, where $X_n = \{\mathbf{x}_t\}_{t=1,\dots,n}$. . The upper bound follows from the standard result that conditioning on more information only decreases the entropy: $H(A|B) \leq H(A)$ for any random variables $A$ and $B$.</p>
<p>We then estimate the resulting lower bound of the PES acquisition function by approximating the intractable distribution $p(\mathbf{y}| \mathbf{x}, D_n, f(X_n \cup \{\mathbf{x}\}) \preceq f(\mathbb{X}^*))$ with a product of Gaussian random variables, which is fitted via an iterative moment-matching procedure known as expectation propagation. The entropy of this resulting distribution can then be computed analytically.</p>
<p><strong>MES and JES entropy estimate.</strong> In <code>qLowerBoundMultiObjectiveMaxValueEntropySearch</code>, <code>qLowerBoundJointEntropySearch</code> and <code>qLowerBoundMultiObjectiveJointEntropySearch</code>, we approximate the entropy term arising in MES and JES using the strategies described in [3]. These estimates rely on different upper bounds of the entropy term, which results in different lower bounds for the mutual information. These estimates are motivated by the following chain inequalities for the entropy in the JES expression:</p>
<p>\begin{align}
    H[p(\mathbf{y}| \mathbf{x}, D_n, (\mathbb{X}^*, \mathbb{Y}^*))]
    &amp;\overset{(1)}{=} H[p(\mathbf{y}| \mathbf{x}, D_n \cup (\mathbb{X}^*, \mathbb{Y}^*), f(\mathbb{X}) \preceq \mathbb{Y}^*)]
    \\\\
    &amp;\overset{(2)}{\leq} H[p(\mathbf{y}| \mathbf{x}, D_n \cup (\mathbb{X}^*, \mathbb{Y}^*), f(\mathbf{x}) \preceq \mathbb{Y}^*)]
    \\\\
    &amp;\overset{(3)}{\leq} H[\mathcal{N}(\mathbf{y}| \mathbf{m}_{(\mathbf{x}, (\mathbb{X}^*, \mathbb{Y}^*))}, \mathbf{V}_{(\mathbf{x}, (\mathbb{X}^*, \mathbb{Y}^*))})]
    \\\\
    &amp;\overset{(4)}{\leq} H[\mathcal{N}(\mathbf{y}| \mathbf{m}_{(\mathbf{x}, (\mathbb{X}^*, \mathbb{Y}^*))}, \text{diag}(\mathbf{V}_{(\mathbf{x}, (\mathbb{X}^*, \mathbb{Y}^*))}))],
\end{align}</p>
<p>where</p>
<p>\begin{align}
    \mathbf{m}_{(\mathbf{x}, (\mathbb{X}^*, \mathbb{Y}^*))} = \mathbb{E}[p(\mathbf{y}| \mathbf{x}, D_n \cup (\mathbb{X}^*, \mathbb{Y}^*), f(\mathbf{x}) \preceq \mathbb{Y}^*)]
\end{align}</p>
<p>\begin{align}
    \mathbf{V}_{(\mathbf{x}, (\mathbb{X}^*, \mathbb{Y}^*))} = \mathbb{C}\text{ov}[p(\mathbf{y}| \mathbf{x}, D_n \cup (\mathbb{X}^*, \mathbb{Y}^*), f(\mathbf{x}) \preceq \mathbb{Y}^*)].
\end{align}</p>
<p>(1) This statement follows from the observation that conditioning on the optimal points $(\mathbb{X}^*, \mathbb{Y}^*)$ is equivalent to knowing that $\mathbb{X}^*$ maps to $\mathbb{Y}^*$ and that all points lie below the Pareto front, $f(\mathbb{X}) \preceq f(\mathbb{X}^*) = \mathbb{Y}^*$.</p>
<p>(2) We replace the global optimality condition with the local optimality condition: $f(\mathbf{x}) \preceq \mathbb{Y}^*$. The upper bound follows from the standard result that conditioning on more information only decreases the entropy: $H(A|B) \leq H(A)$ for any random variables $A$ and $B$.</p>
<p>(3) We upper bound the entropy using the standard result that the multivariate Gaussian distribution has the maximum entropy over all distributions supported on $\mathbb{R}^M$ with the same first two moments.</p>
<p>(4) We upper bound the entropy by again using the standard result that conditioning on more information only decreases the entropy.</p>
<p><strong>(Conditioning)</strong> A similar chain of inequalities can be obtained for the entropy in the MES term by replacing the augmented data set $D_n \cup (\mathbb{X}^*, \mathbb{Y}^*)$ with the original data set $D_n$. The only real difference between the JES and MES estimate is whether we condition on the extra samples $(\mathbb{X}^*_j, \mathbb{Y}^*_j)$ or not for $j=1,\dots,J$. As a result of this conditioning, the JES estimate can be more expensive than the MES estimate.</p>
<p><strong>(Noiseless setting)</strong> When the observations are exact, $\mathbf{y} = f(\mathbf{x})$, then the entropy term in (2) can be computed exactly. By setting <code>estimation_type="0"</code>, we use this estimate. In the setting where there is observation noise, the estimate also includes an ad-hoc correction which can be useful (more details in the appendix of [3]).</p>
<p><strong>(Monte Carlo)</strong> The entropy term in (2) can be estimated using Monte Carlo because the distribution has a tractable density under the assumptions. By setting <code>estimation_type="MC"</code>, we use this Monte Carlo estimate.</p>
<p><strong>(Lower bound)</strong> The entropy term in (3) and (4) can be computed exactly. By setting <code>estimation_type="LB"</code>, we use this lower bound estimate in (3). By setting <code>estimation_type="LB2"</code>, we use lower bound estimate in (4).</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=658c9cc3"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Batch">Batch<a class="anchor-link" href="#Batch">¶</a></h3><p>For the batch setting, the first term is again analytically tractable. The second term can be estimated using Monte Carlo, whilst the entropy term again has to be estimated.</p>
<p><strong>PES entropy estimate.</strong> In <code>qPredictiveEntropySearch</code> and <code>qMultiObjectivePredictiveEntropySearch</code>, the entropy term is again approximated using expectation propagation. In particular, we approximate $p(Y| X, D_n, f(X_n \cup X) \preceq f(\mathbb{X}^*))$ with a product of Gaussian random variables.</p>
<p><strong>MES and JES entropy estimate</strong> In <code>qLowerBoundMultiObjectiveMaxValueEntropySearch</code>, <code>qLowerBoundJointEntropySearch</code> and <code>qLowerBoundMultiObjectiveJointEntropySearch</code>, we approximate a lower bound to the MES and JES acquisition function:</p>
<p>\begin{equation}
\alpha^{\text{LB-MES}}(X|D_n) 
= \text{MI}(Y; \mathbb{Y}^*| X, D_n) 
= H[p(Y|D_n)] - \sum_{\mathbf{x} \in X} \mathbb{E}_{p(\mathbb{Y}^*|D_n)}[H[p(\mathbf{y}| \mathbf{x}, D_n, \mathbb{Y}^*)]],
\end{equation}</p>
<p>\begin{equation}
\alpha^{\text{LB-JES}}(X|D_n) 
= \text{MI}(Y; (\mathbb{X}^*, \mathbb{Y}^*)| X, D_n) 
= H[p(Y|D_n)] - \sum_{\mathbf{x} \in X} \mathbb{E}_{p((\mathbb{X}^*, \mathbb{Y}^*)|D_n)}[H[p(\mathbf{y}| \mathbf{x}, D_n, (\mathbb{X}^*, \mathbb{Y}^*))]].
\end{equation}</p>
<p>The advantage of these expressions is that it allows us to take advantage of the existing entropy estimates for the sequential setting.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=1ce75823"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References">References<a class="anchor-link" href="#References">¶</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=262131d3"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>[1] J.M. Hernández-Lobato, M.W. Hoffman and Z. Ghahramani, <a href="https://arxiv.org/abs/1406.2541"><strong>Predictive Entropy Search for Efficient Global Optimization of Black-box Functions</strong></a>, NeurIPS, 2014.</p>
<p>[2] Z. Wang and S. Jegelka, <a href="https://arxiv.org/abs/1703.01968"><strong>Max-value Entropy Search for Efficient Bayesian Optimization</strong></a>, ICML, 2017.</p>
<p>[3] B. Tu, A. Gandy, N. Kantas and B. Shafei, <a href="https://arxiv.org/abs/2210.02905"><strong>Joint Entropy Search for Multi-Objective Bayesian Optimization</strong></a>, NeurIPS, 2022.</p>
<p>[4] C. Hvarfner, F. Hutter and N. Nardi, <a href="https://arxiv.org/abs/2206.04771"><strong>Joint Entropy Search for Maximally-Informed Bayesian Optimization</strong></a>, NeurIPS, 2022.</p>
<p>[5] E. Garrido-Merchán and D. Hernández-Lobato, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231219308525"><strong>Predictive Entropy Search for Multi-objective Bayesian Optimization with Constraints</strong></a>, Neurocomputing, 2019.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=7490ac1c"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="1.-Single-objective-example">1. Single-objective example<a class="anchor-link" href="#1.-Single-objective-example">¶</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=5c3e4976"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this section, we present a simple example in one-dimension with one objective to illustrate the use of these acquisition functions. We first define the objective function.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=908e289f">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">botorch.fit</span> <span class="kn">import</span> <span class="n">fit_gpytorch_mll</span>
<span class="kn">from</span> <span class="nn">botorch.models.gp_regression</span> <span class="kn">import</span> <span class="n">SingleTaskGP</span>
<span class="kn">from</span> <span class="nn">botorch.models.transforms.outcome</span> <span class="kn">import</span> <span class="n">Standardize</span>
<span class="kn">from</span> <span class="nn">botorch.utils.sampling</span> <span class="kn">import</span> <span class="n">draw_sobol_samples</span>
<span class="kn">from</span> <span class="nn">gpytorch.mlls.exact_marginal_log_likelihood</span> <span class="kn">import</span> <span class="n">ExactMarginalLogLikelihood</span>

<span class="n">SMOKE_TEST</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"SMOKE_TEST"</span><span class="p">)</span>
<span class="n">tkwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"dtype"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">,</span> <span class="s2">"device"</span><span class="p">:</span> <span class="s2">"cpu"</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">p2</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">p3</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">p4</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p1</span> <span class="o">+</span> <span class="n">p2</span> <span class="o">+</span> <span class="n">p3</span> <span class="o">+</span> <span class="n">p4</span>


<span class="n">bounds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">]],</span> <span class="o">**</span><span class="n">tkwargs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=0df52007"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now generate some data and then fit the Gaussian process model.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=5770f703">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">train_X</span> <span class="o">=</span> <span class="n">draw_sobol_samples</span><span class="p">(</span><span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">12345678</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
<span class="n">train_Y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">train_X</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">fit_model</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">SingleTaskGP</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">outcome_transform</span><span class="o">=</span><span class="n">Standardize</span><span class="p">(</span><span class="n">m</span><span class="o">=</span><span class="n">num_outputs</span><span class="p">))</span>
    <span class="n">mll</span> <span class="o">=</span> <span class="n">ExactMarginalLogLikelihood</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
    <span class="n">fit_gpytorch_mll</span><span class="p">(</span><span class="n">mll</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">fit_model</span><span class="p">(</span><span class="n">train_X</span><span class="o">=</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="o">=</span><span class="n">train_Y</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=0b6b02f9"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now plot the objective function and the model.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=877a342b">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">bounds</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bounds</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="mi">1000</span><span class="p">,</span> <span class="o">**</span><span class="n">tkwargs</span><span class="p">)</span>
<span class="n">mean_fX</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">posterior</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">std_fX</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">posterior</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">variance</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"k"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Observations"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">"k"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Objective function"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">mean_fX</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"dodgerblue"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Posterior model"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="p">(</span><span class="n">mean_fX</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">std_fX</span><span class="p">),</span> <span class="p">(</span><span class="n">mean_fX</span> <span class="o">-</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">std_fX</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"dodgerblue"</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=6ec0e247"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To compute the information-theoretic acquisition functions, we first need to get some Monte Carlo samples of the optimal inputs and outputs. The method <code>sample_optimal_points</code> generates <code>num_samples</code> approximate samples of the Gaussian process model and optimizes them sequentially using an optimizer. In the single-objective setting, the number of optimal points (<code>num_points</code>) should be set to one. For simplicitly, we consider optimization via random search.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=79e93848">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">botorch.acquisition.utils</span> <span class="kn">import</span> <span class="n">get_optimal_samples</span>

<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">12</span>

<span class="n">optimal_inputs</span><span class="p">,</span> <span class="n">optimal_outputs</span> <span class="o">=</span> <span class="n">get_optimal_samples</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">num_optima</span><span class="o">=</span><span class="n">num_samples</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=620d538a"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now initialize the information-theoretic acquisition functions. The PES can simply be initialized using just the optimal set of inputs. For the MES and JES acquisition function, we also have to specify the region of integration, which is $\{\mathbf{y}: \mathbf{y} \preceq \mathbb{Y}^*\}$ for a maximization problem. This is done by providing a Tensor of bounds, which is obtained via the method <code>compute_sample_box_decomposition</code>.</p>
<p>Note that for the MES algorithm, we use the multi-objective implementation <code>qLowerBoundMultiObjectiveMaxValueEntropySearch</code>, which implements all the estimation types into one acquisition function. BoTorch alreadys supports many other strategies to estimate the single-objective MES algorithms in <code>botorch.acquisition.max_value_entropy</code>, which is described in the other complementary notebooks.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=320b07cc">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">botorch.acquisition.joint_entropy_search</span> <span class="kn">import</span> <span class="n">qJointEntropySearch</span>
<span class="kn">from</span> <span class="nn">botorch.acquisition.max_value_entropy_search</span> <span class="kn">import</span> <span class="n">qLowerBoundMaxValueEntropy</span>
<span class="kn">from</span> <span class="nn">botorch.acquisition.predictive_entropy_search</span> <span class="kn">import</span> <span class="n">qPredictiveEntropySearch</span>

<span class="n">pes</span> <span class="o">=</span> <span class="n">qPredictiveEntropySearch</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">optimal_inputs</span><span class="o">=</span><span class="n">optimal_inputs</span><span class="p">)</span>

<span class="c1"># Here we use the lower bound estimates for the MES and JES</span>
<span class="c1"># Note that the single-objective MES interface is slightly different,</span>
<span class="c1"># as it utilizes the Gumbel max-value approximation internally and</span>
<span class="c1"># therefore does not take the max values as input.</span>
<span class="n">mes_lb</span> <span class="o">=</span> <span class="n">qLowerBoundMaxValueEntropy</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">candidate_set</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">jes_lb</span> <span class="o">=</span> <span class="n">qJointEntropySearch</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">optimal_inputs</span><span class="o">=</span><span class="n">optimal_inputs</span><span class="p">,</span>
    <span class="n">optimal_outputs</span><span class="o">=</span><span class="n">optimal_outputs</span><span class="p">,</span>
    <span class="n">estimation_type</span><span class="o">=</span><span class="s2">"LB"</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=ec4692e9"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To illustrate the acquisition functions, we evaluate it over the whole input space and plot it. As described in [3], the JES should be an upper bound to both the PES and MES, although the estimates might not be.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=382e37f4">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># the acquisition function call takes a three-dimensional tensor</span>
<span class="n">fwd_X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># make the acquisition functions live on the same scale</span>
<span class="n">scale_acqvals</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">pes_X</span> <span class="o">=</span> <span class="n">pes</span><span class="p">(</span><span class="n">fwd_X</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">mes_lb_X</span> <span class="o">=</span> <span class="n">mes_lb</span><span class="p">(</span><span class="n">fwd_X</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">jes_lb_X</span> <span class="o">=</span> <span class="n">jes_lb</span><span class="p">(</span><span class="n">fwd_X</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="k">if</span> <span class="n">scale_acqvals</span><span class="p">:</span>
    <span class="n">pes_X</span> <span class="o">=</span> <span class="n">pes_X</span> <span class="o">/</span> <span class="n">pes_X</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">mes_lb_X</span> <span class="o">=</span> <span class="n">mes_lb_X</span> <span class="o">/</span> <span class="n">mes_lb_X</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">jes_lb_X</span> <span class="o">=</span> <span class="n">jes_lb_X</span> <span class="o">/</span> <span class="n">jes_lb_X</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pes_X</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"mediumseagreen"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"PES"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">mes_lb_X</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"crimson"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"MES-LB"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">jes_lb_X</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"dodgerblue"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"JES-LB"</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span>
    <span class="n">X</span><span class="p">[</span><span class="n">pes_X</span><span class="o">.</span><span class="n">argmax</span><span class="p">()],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"mediumseagreen"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">"--"</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">mes_lb_X</span><span class="o">.</span><span class="n">argmax</span><span class="p">()],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"crimson"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">":"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span>
    <span class="n">X</span><span class="p">[</span><span class="n">jes_lb_X</span><span class="o">.</span><span class="n">argmax</span><span class="p">()],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"dodgerblue"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">"--"</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"$x$"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">"$\alpha(x)$"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Entropy-based acquisition functions"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=3ce0f584"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To maximize the acquisition function in a standard Bayesian optimization loop, we can use the standard optimization routines. Note that the PES acquisition function might not be differentiable since some operations that may arise during expectation propagation are not differentiable. Therefore, we use a finite difference approach to optimize this acquisition function.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=f7f639bb">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">botorch.optim</span> <span class="kn">import</span> <span class="n">optimize_acqf</span>

<span class="c1"># Use finite difference for PES</span>
<span class="n">candidate</span><span class="p">,</span> <span class="n">acq_value</span> <span class="o">=</span> <span class="n">optimize_acqf</span><span class="p">(</span>
    <span class="n">acq_function</span><span class="o">=</span><span class="n">pes</span><span class="p">,</span>
    <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
    <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">num_restarts</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">raw_samples</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">"with_grad"</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"PES: candidate=</span><span class="si">{}</span><span class="s2">, acq_value=</span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">candidate</span><span class="p">,</span> <span class="n">acq_value</span><span class="p">))</span>

<span class="n">candidate</span><span class="p">,</span> <span class="n">acq_value</span> <span class="o">=</span> <span class="n">optimize_acqf</span><span class="p">(</span>
    <span class="n">acq_function</span><span class="o">=</span><span class="n">mes_lb</span><span class="p">,</span>
    <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
    <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">num_restarts</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">raw_samples</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"MES-LB: candidate=</span><span class="si">{}</span><span class="s2">, acq_value=</span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">candidate</span><span class="p">,</span> <span class="n">acq_value</span><span class="p">))</span>

<span class="n">candidate</span><span class="p">,</span> <span class="n">acq_value</span> <span class="o">=</span> <span class="n">optimize_acqf</span><span class="p">(</span>
    <span class="n">acq_function</span><span class="o">=</span><span class="n">jes_lb</span><span class="p">,</span>
    <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
    <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">num_restarts</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">raw_samples</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"JES-LB: candidate=</span><span class="si">{}</span><span class="s2">, acq_value=</span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">candidate</span><span class="p">,</span> <span class="n">acq_value</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=e95f0846"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="2.-Multi-objective-batch-example">2. Multi-objective batch example<a class="anchor-link" href="#2.-Multi-objective-batch-example">¶</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=57237806"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this section, we illustrate a simple multi-objective example. First we generate some data and fit the model.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=fabc86e9">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">botorch.acquisition.multi_objective.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">compute_sample_box_decomposition</span><span class="p">,</span>
    <span class="n">random_search_optimizer</span><span class="p">,</span>
    <span class="n">sample_optimal_points</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">botorch.test_functions.multi_objective</span> <span class="kn">import</span> <span class="n">ZDT1</span>

<span class="n">d</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">8</span>

<span class="k">if</span> <span class="n">SMOKE_TEST</span><span class="p">:</span>
    <span class="n">q</span> <span class="o">=</span> <span class="mi">2</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">q</span> <span class="o">=</span> <span class="mi">4</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=34787908">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">problem</span> <span class="o">=</span> <span class="n">ZDT1</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">num_objectives</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">noise_std</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">negate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">bounds</span> <span class="o">=</span> <span class="n">problem</span><span class="o">.</span><span class="n">bounds</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="o">**</span><span class="n">tkwargs</span><span class="p">)</span>

<span class="n">train_X</span> <span class="o">=</span> <span class="n">draw_sobol_samples</span><span class="p">(</span><span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
<span class="n">train_Y</span> <span class="o">=</span> <span class="n">problem</span><span class="p">(</span><span class="n">train_X</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">fit_model</span><span class="p">(</span><span class="n">train_X</span><span class="o">=</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="o">=</span><span class="n">train_Y</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=b7174710"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now obtain Monte Carlo samples of the optimal inputs and outputs.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=56bd5f5a">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">num_pareto_samples</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">num_pareto_points</span> <span class="o">=</span> <span class="mi">8</span>

<span class="c1"># We set the parameters for the random search</span>
<span class="n">optimizer_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"pop_size"</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s2">"max_tries"</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">ps</span><span class="p">,</span> <span class="n">pf</span> <span class="o">=</span> <span class="n">sample_optimal_points</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
    <span class="n">num_samples</span><span class="o">=</span><span class="n">num_pareto_samples</span><span class="p">,</span>
    <span class="n">num_points</span><span class="o">=</span><span class="n">num_pareto_points</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">random_search_optimizer</span><span class="p">,</span>
    <span class="n">optimizer_kwargs</span><span class="o">=</span><span class="n">optimizer_kwargs</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=76c35b23"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We initialize the acquisition functions as before.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=2c7dfaf0">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">botorch.acquisition.multi_objective.joint_entropy_search</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">qLowerBoundMultiObjectiveJointEntropySearch</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">botorch.acquisition.multi_objective.max_value_entropy_search</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">qLowerBoundMultiObjectiveMaxValueEntropySearch</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">botorch.acquisition.multi_objective.predictive_entropy_search</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">qMultiObjectivePredictiveEntropySearch</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">pes</span> <span class="o">=</span> <span class="n">qMultiObjectivePredictiveEntropySearch</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">pareto_sets</span><span class="o">=</span><span class="n">ps</span><span class="p">)</span>

<span class="c1"># Compute the box-decomposition</span>
<span class="n">hypercell_bounds</span> <span class="o">=</span> <span class="n">compute_sample_box_decomposition</span><span class="p">(</span><span class="n">pf</span><span class="p">)</span>

<span class="c1"># # Here we use the lower bound estimates for the MES and JES</span>
<span class="n">mes_lb</span> <span class="o">=</span> <span class="n">qLowerBoundMultiObjectiveMaxValueEntropySearch</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">hypercell_bounds</span><span class="o">=</span><span class="n">hypercell_bounds</span><span class="p">,</span>
    <span class="n">estimation_type</span><span class="o">=</span><span class="s2">"LB"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">jes_lb</span> <span class="o">=</span> <span class="n">qLowerBoundMultiObjectiveJointEntropySearch</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">pareto_sets</span><span class="o">=</span><span class="n">ps</span><span class="p">,</span>
    <span class="n">pareto_fronts</span><span class="o">=</span><span class="n">pf</span><span class="p">,</span>
    <span class="n">hypercell_bounds</span><span class="o">=</span><span class="n">hypercell_bounds</span><span class="p">,</span>
    <span class="n">estimation_type</span><span class="o">=</span><span class="s2">"LB"</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered" id="cell-id=6a6d071b"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now optimize the batch acquistion functions. For the batch PES, we optimize the batch acquisition function directly. Whereas for the MES and JES we use a sequential optimization strategy.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=ceac58f5">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>
<span class="c1"># Use finite difference for PES. This may take some time</span>
<span class="n">candidates</span><span class="p">,</span> <span class="n">acq_values</span> <span class="o">=</span> <span class="n">optimize_acqf</span><span class="p">(</span>
    <span class="n">acq_function</span><span class="o">=</span><span class="n">pes</span><span class="p">,</span>
    <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
    <span class="n">q</span><span class="o">=</span><span class="n">q</span><span class="p">,</span>
    <span class="n">num_restarts</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">raw_samples</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">"with_grad"</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"PES: </span><span class="se">\n</span><span class="s2">candidates=</span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">candidates</span><span class="p">))</span>

<span class="c1"># Sequentially greedy optimization</span>
<span class="n">candidates</span><span class="p">,</span> <span class="n">acq_values</span> <span class="o">=</span> <span class="n">optimize_acqf</span><span class="p">(</span>
    <span class="n">acq_function</span><span class="o">=</span><span class="n">mes_lb</span><span class="p">,</span>
    <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
    <span class="n">q</span><span class="o">=</span><span class="n">q</span><span class="p">,</span>
    <span class="n">num_restarts</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">raw_samples</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">sequential</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"MES-LB: </span><span class="se">\n</span><span class="s2">candidates=</span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">candidates</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered" id="cell-id=d9281308">
<div class="input">
<div class="prompt input_prompt">In [ ]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Sequentially greedy optimization</span>
<span class="n">candidates</span><span class="p">,</span> <span class="n">acq_values</span> <span class="o">=</span> <span class="n">optimize_acqf</span><span class="p">(</span>
    <span class="n">acq_function</span><span class="o">=</span><span class="n">jes_lb</span><span class="p">,</span>
    <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
    <span class="n">q</span><span class="o">=</span><span class="n">q</span><span class="p">,</span>
    <span class="n">num_restarts</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">raw_samples</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">sequential</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"JES-LB: </span><span class="se">\n</span><span class="s2">candidates=</span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">candidates</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>